{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reduce` function reduces the dimensionality of an array or list of arrays. The default is to use Principal Component Analysis to redue to three dimensions, but a variety of models are supported and users may specify a desired number of dimensions other than three.\n",
    "\n",
    "Supported models include: PCA, IncrementalPCA, SparsePCA, MiniBatchSparsePCA, KernelPCA, FastICA, FactorAnalysis, TruncatedSVD, DictionaryLearning, MiniBatchDictionaryLearning, TSNE, Isomap, SpectralEmbedding, LocallyLinearEmbedding, and MDS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Hypertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hypertools as hyp\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load your data\n",
    "\n",
    "First, we'll load one of the sample datasets. This dataset is a list of 2 `numpy` arrays, each containing average brain activity (fMRI) from 18 subjects listening to the same story, fit using Hierarchical Topographic Factor Analysis (HTFA) with 100 nodes.  The rows are timepoints and the columns are fMRI components. \n",
    "\n",
    "See the [full dataset](http://dataspace.princeton.edu/jspui/handle/88435/dsp015d86p269k) or the [HTFA article](https://www.biorxiv.org/content/early/2017/02/07/106690) for more info on the data and HTFA, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = hyp.load('weights_avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce one array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one array from the dataset above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array shape: (300, 100)\n"
     ]
    }
   ],
   "source": [
    "print('Array shape: (%d, %d)' % weights[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce this array, simply pass the array to `hyp.reduce`, as below. We can see that the data has been reduced from 100 features to 3 features (the default when desired number of features is not specified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced array shape: (300, 3)\n"
     ]
    }
   ],
   "source": [
    "reduced_array = hyp.reduce(weights[0])\n",
    "print('Reduced array shape: (%d, %d)' % reduced_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce list of arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list or numpy array of multiple arrays can also be reduced into a common space. That is, the data can be combined, reduced as a whole, then split back into individual elements and outputted via hyp.reduce.  \n",
    "\n",
    "Here we show this with two arrays in the weights dataset. First, let's examine the arrays in the weights dataset (below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's reduce both arrays at once (by passing in the whole of the weights data) and re-examine the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of first reduced array:  (300, 3)\n",
      "Shape of second reduced array:  (300, 3)\n"
     ]
    }
   ],
   "source": [
    "reduced_arrays = hyp.reduce(weights)\n",
    "print('Shape of first reduced array: ', reduced_arrays[0].shape)\n",
    "print('Shape of second reduced array: ', reduced_arrays[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each array has been reduced from 100 features to 3 features (the default when desired number of features is not specified), with the number of datapoints unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce list of arrays (TSNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also opt to use different reduction methods.  In the example below, we reduce multiple arrays at once, using TSNE. The data is reduced to three dimensions(the default when desired number of features not specified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of first reduced array:  (300, 3)\n",
      "Shape of second reduced array:  (300, 3)\n"
     ]
    }
   ],
   "source": [
    "reduced_TSNE = hyp.reduce(weights, reduce='TSNE')\n",
    "print('Shape of first reduced array: ',reduced_TSNE[0].shape)\n",
    "print('Shape of second reduced array: ',reduced_TSNE[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce to specified number of dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may prefer to reduce to a specific number of features, rather than defaulting the three dimensions.  To achieve this, simply pass the number of desired features (as an int) to the ndims argument, as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of first reduced array:  (300, 4)\n",
      "Shape of second reduced array:  (300, 4)\n"
     ]
    }
   ],
   "source": [
    "reduced_4 = hyp.reduce(weights, ndims = 4)\n",
    "print('Shape of first reduced array: ', reduced_4[0].shape)\n",
    "print('Shape of second reduced array: ', reduced_4[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce list of arrays with specific parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For finer control of parameters, a dictionary of model parameters may be passed to the reduce argument, in addition to the desired reduction method. See [scikit-learn](http://scikit-learn.org/stable/index.html) model docs for details on parameters supported for each model.\n",
    "\n",
    "Supported models include: PCA, IncrementalPCA, SparsePCA, MiniBatchSparsePCA, KernelPCA, FastICA, FactorAnalysis, TruncatedSVD, DictionaryLearning, MiniBatchDictionaryLearning, TSNE, Isomap, SpectralEmbedding, LocallyLinearEmbedding, and MDS.\n",
    "\n",
    "The example below will reduce to the default of three features, since the desired number of features is not specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_params = hyp.reduce(weights, reduce={'model' : 'PCA', 'params' : {'whiten' : True}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
