{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import modin.pandas as pd\n",
    "import pandas as pd\n",
    "from ppca import PPCA\n",
    "import six\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, AgglomerativeClustering, Birch, FeatureAgglomeration, SpectralClustering, SpectralCoclustering, SpectralBiclustering, DBSCAN, AffinityPropagation, MeanShift\n",
    "from sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n",
    "from sklearn.decomposition import PCA, FastICA, IncrementalPCA, KernelPCA, FactorAnalysis, TruncatedSVD, SparsePCA, MiniBatchSparsePCA, DictionaryLearning, MiniBatchDictionaryLearning\n",
    "from sklearn.manifold import TSNE, MDS, SpectralEmbedding, LocallyLinearEmbedding, Isomap\n",
    "from umap import UMAP\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "#import plotly_express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0517 22:55:50.494333 4705039808 deprecation.py:323] From /usr/local/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:63: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "#import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "universal_sentence_encoder = hub.Module('universal-sentence-encoder-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = ['cat', 'dog', 'elephant', 'goose', 'paper', 'how much wood could a woodchuck chuck', 'if a woodchuck could chuck wood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_vectorizer(text, model=universal_sentence_encoder):\n",
    "    if not (type(text) == list):\n",
    "        text = [text]\n",
    "    \n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])        \n",
    "        return session.run(model(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = text_vectorizer(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 512)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "#from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults = ConfigParser()\n",
    "defaults.read('defaults.ini');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_models = ['DictionaryLearning', 'FactorAnalysis', 'FastICA', 'IncrementalPCA', 'KernelPCA', 'LatentDirichletAllocation', 'MiniBatchDictionaryLearning',\n",
    " 'MiniBatchSparsePCA', 'NMF', 'PCA', 'SparseCoder', 'SparsePCA', 'TruncatedSVD', 'UMAP', 'TSNE', 'MDS', 'SpectralEmbedding', 'LocallyLinearEmbedding', 'Isomap']\n",
    "\n",
    "cluster_models = ['AffinityPropagation', 'AgglomerativeClustering', 'Birch', 'DBSCAN', 'FeatureAgglomeration', 'KMeans', 'MeanShift', 'MiniBatchKMeans', 'SpectralBiclustering', 'SpectralClustering', 'SpectralCoclustering', 'DBSCAN', 'AffinityPropagation', 'MeanShift']\n",
    "mixture_models = ['GaussianMixture', 'BayesianGaussianMixture', 'LatentDirichletAllocation', 'NMF']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dataframe(x):\n",
    "    return type(x).__module__ in ['pandas.core.frame', 'modin.pandas.dataframe']\n",
    "\n",
    "def is_array(x):\n",
    "    return type(x).__module__ == 'numpy'\n",
    "\n",
    "def is_empty(x):\n",
    "    return (x is None) or (len(x) == 0)\n",
    "\n",
    "def is_text(x):\n",
    "    if type(x) == list:\n",
    "        return np.all([is_text(t) for t in x])\n",
    "    return type(x) in six.string_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandify(idx, cols, vals, force=False):\n",
    "    if (len(idx))\n",
    "    \n",
    "    if (len(idx) == 0) or (idx is None):\n",
    "        idx = np.arange(vals.shape[0])\n",
    "    if (len(cols) == 0) or (cols is None):\n",
    "        cols = np.arange(ndims)\n",
    "    \n",
    "    if is_dataframe(vals):\n",
    "        vals = vals.copy()\n",
    "        vals.index = idx\n",
    "        vals.columns = cols\n",
    "        return vals\n",
    "    else:\n",
    "        return \n",
    "    \n",
    "\n",
    "def depandify(x):\n",
    "    if is_dataframe(data):\n",
    "        idx = data.index\n",
    "        cols = np.arange(ndims)\n",
    "        vals = data.values\n",
    "    elif is_array(data):\n",
    "        idx = []\n",
    "        cols = []\n",
    "        vals = data\n",
    "    else:\n",
    "        raise Exception(f'Unsupported data type: {type(data)}')\n",
    "    return idx, cols, vals\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer(data=None, algorithm='IncrementalPCA', ndims=3, fill_missing=False, force_pandas=False, **kwargs):\n",
    "    '''\n",
    "    ARGUMENTS:\n",
    "    data: data to reduce (numpy array or compatable, or a pandas\n",
    "          dataframe or compatable).  Formatted as a 2d matrix whose\n",
    "          rows are observations and whose columns are feature\n",
    "          dimensions.\n",
    "    \n",
    "    force_pandas: if True, force the result to be a pandas dataframe;\n",
    "          if False (default), return the inputs in the same format\n",
    "          as they were provided (unless stack=True, in which case\n",
    "          force_pandas is reset to True).\n",
    "    \n",
    "    algorithm: one of: 'DictionaryLearning', 'FactorAnalysis',\n",
    "          'FastICA', 'IncrementalPCA', 'KernelPCA',\n",
    "          'LatentDirichletAllocation', 'MiniBatchDictionaryLearning',\n",
    "          'MiniBatchSparsePCA', 'NMF', 'PCA', 'SparseCoder',\n",
    "          'SparsePCA', 'TruncatedSVD', or 'UMAP'.  Calls the\n",
    "          relevant scikit-learn or UMAP function.  Can also\n",
    "          pass a function directly.\n",
    "    \n",
    "    ndims: the number of dimensions (columns) in the result (default: 3).\n",
    "          if ndims > data.shape[1], the right-most columns will be\n",
    "          zero-padded.\n",
    "    \n",
    "    fill_missing: if True, use PPCA to fill in nan-valued entries\n",
    "    \n",
    "    all additional keyword arguments are passed to the reduce algorithm\n",
    "    \n",
    "    RETURNS:\n",
    "    numpy array or pandas dataframe with number-of-observations rows and\n",
    "    ndims columns\n",
    "    '''\n",
    "    if type(algorithm) == str:\n",
    "        assert algorithm in reduce_models, f'Unknown algorithm: {algorithm}' #security check to prevent executing arbitrary code\n",
    "        algorithm = eval(algorithm)\n",
    "    else:\n",
    "        if ('sklearn.decomposition' in algorithm.__module__) or ('umap.umap_' in algorithm.__module__):\n",
    "            pass #algorithm already given as a function\n",
    "    assert ndims >= 0, f'Number of dimensions must be non-negative (given: {ndims})'\n",
    "    \n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "    \n",
    "    if fill_missing:\n",
    "        raise(NotImplementedError('call PPCA and/or use interpolation to fill in missing values'))\n",
    "    \n",
    "    if data.shape == ndims:\n",
    "        return data\n",
    "    \n",
    "    if is_dataframe(data):\n",
    "        idx = data.index\n",
    "        cols = np.arange(ndims)\n",
    "        vals = data.values\n",
    "    elif is_array(data):\n",
    "        idx = []\n",
    "        cols = []\n",
    "        vals = data\n",
    "    else:\n",
    "        raise Exception(f'Unsupported data type: {type(data)}')\n",
    "    \n",
    "    if force_pandas:\n",
    "        if len(idx) == 0:\n",
    "            idx = np.arange(vals.shape[0])\n",
    "        if len(cols) == 0:\n",
    "            cols = np.arange(ndims)\n",
    "    \n",
    "    if vals.shape[1] > ndims:\n",
    "        model = algorithm(n_components=ndims, **kwargs)\n",
    "        reduced_vals = model.fit_transform(vals)\n",
    "    elif vals.shape[1] < ndims:\n",
    "        reduced_vals = np.concatenate([vals, np.zeros([vals.shape[0], ndims-vals.shape[1]])], axis=1)\n",
    "    else:\n",
    "        reduced_vals = vals\n",
    "    \n",
    "    if is_dataframe(data) or force_pandas:\n",
    "        return pd.DataFrame(data=reduced_vals, index=idx, columns=cols)\n",
    "    else:\n",
    "        return reduced_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_reducer(data=None, stack=False, names=None, **kwargs):\n",
    "    '''\n",
    "    ARGUMENTS:\n",
    "    data: a numpy array (or compatable), pandas dataframe \n",
    "          (or compatable), or a list of arrays and/or dataframes.\n",
    "          If multiple matrices are passed (e.g. in a list), they\n",
    "          must all have the same number of columns.\n",
    "    \n",
    "    stack: should the answer be returned as a list (stack=False,\n",
    "          default) or a single multiindexed pandas dataframe\n",
    "          (stack=True)?\n",
    "    \n",
    "    names: a list or array of strings, of the same length as data\n",
    "          (or, if data isn't a list, just a single string),\n",
    "          describing each dataset (e.g. for use in a legend)\n",
    "    \n",
    "    force_pandas: if True, force the result to be a pandas dataframe;\n",
    "          if False (default), return the inputs in the same format\n",
    "          as they were provided (unless stack=True, in which case\n",
    "          force_pandas is reset to True).\n",
    "    \n",
    "    algorithm: one of: 'DictionaryLearning', 'FactorAnalysis',\n",
    "          'FastICA', 'IncrementalPCA', 'KernelPCA',\n",
    "          'LatentDirichletAllocation', 'MiniBatchDictionaryLearning',\n",
    "          'MiniBatchSparsePCA', 'NMF', 'PCA', 'SparseCoder',\n",
    "          'SparsePCA', 'TruncatedSVD', or 'UMAP'.  Calls the\n",
    "          relevant scikit-learn or UMAP function.  Can also\n",
    "          pass a function directly.\n",
    "    \n",
    "    all additional keyword arguments are passed to the reduce algorithm\n",
    "    \n",
    "    RETURNS:\n",
    "    \n",
    "    One or more reduced-dimensional matrices (each of shape\n",
    "    number-of-observations by ndims), in the same format as the inputted\n",
    "    data (or as pandas dataframes if force_pandas is True).  Or, if \n",
    "    stack=True, return a single multilevel dataframe.    \n",
    "    '''\n",
    "    if not type(data) == list:\n",
    "        reduced = batch_reducer(data=[data], stack=stack, names=names, **kwargs)\n",
    "        if stack:\n",
    "            return reduced\n",
    "        else:\n",
    "            return reduced[0]\n",
    "    \n",
    "    assert len(np.unique([x.shape[1] for x in data])) == 1, 'All datasets must have the same number of columns'\n",
    "    assert np.all([np.ndim(x) == 2 for x in data]), 'All datasets must be 2d arrays, matrices, or DataFrames'\n",
    "    \n",
    "    rows = list(np.cumsum([x.shape[0] for x in data]))\n",
    "    rows.insert(0, 0)\n",
    "    \n",
    "    #there's probably a cleaner way to do this...\n",
    "    if (('force_pandas' in kwargs.keys()) and kwargs['force_pandas']) or stack:\n",
    "        force_pandas = True\n",
    "        for i, d in enumerate(data):\n",
    "            if not is_dataframe(d):\n",
    "                data[i] = pd.DataFrame(data=d, index=np.arange(d.shape[0]), columns=np.arange(d.shape[1]))\n",
    "    else:\n",
    "        force_pandas = False\n",
    "    \n",
    "    modules = [type(x).__module__ for x in data]\n",
    "    idx = []\n",
    "    for i, d in enumerate(data):\n",
    "        if is_dataframe(d):\n",
    "            idx.append(d.index)\n",
    "            data[i] = data[i].values\n",
    "        else:\n",
    "            idx.append([])\n",
    "    \n",
    "    x_combined = reducer(np.concatenate(data, axis=0), **kwargs)\n",
    "    if names is None:\n",
    "        names = np.arange(len(data), dtype=np.int)\n",
    "    \n",
    "    if stack: #put everything into a multilevel index\n",
    "        groups = np.zeros(x_combined.shape[0])\n",
    "        for i, j in enumerate(rows[:-1]):\n",
    "            groups[j:rows[i+1]] = names[i]\n",
    "        idx = pd.MultiIndex.from_arrays([groups, [j for i in idx for j in i]], names=('ID', 'Observation'))\n",
    "        return pd.DataFrame(data=x_combined.values, index=idx, columns=np.arange(x_combined.shape[1]))\n",
    "    \n",
    "    x_split = []\n",
    "    for i, j in enumerate(rows[:-1]):\n",
    "        if is_dataframe(x_combined):\n",
    "            x = x_combined.iloc[j:rows[i+1], :].values\n",
    "        else:\n",
    "            x = x_combined[j:rows[i+1], :]\n",
    "        \n",
    "        if is_dataframe(data[i]):\n",
    "            x_split.append(pd.DataFrame(data=x, index=idx[i], columns=np.arange(x.shape[1])))\n",
    "        else:\n",
    "            x_split.append(x)\n",
    "    return x_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterer(data=None, algorithm='IncrementalPCA', ndims=3, fill_missing=False, force_pandas=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(data, style='line', reduce_kwargs={}, **kwargs):\n",
    "        \n",
    "    def combo_merge(a, b):\n",
    "        combos = []\n",
    "        for i in a:\n",
    "            for j in b:\n",
    "                if (len(i) <= 2):\n",
    "                    if (len(j) <= 2):\n",
    "                        combos.append(i+j)\n",
    "                elif len(j) > 1:\n",
    "                    combos.append(i + '+' + j)\n",
    "        return combos\n",
    "    \n",
    "    marker_styles = ['.', 'o', 'scatter', 'marker', 'markers', 'bigmarker', 'bigmarkers']\n",
    "    line_styles = ['-', '--', ':', 'line', 'lines']\n",
    "    combo_styles = combo_merge(marker_styles, line_styles) + combo_merge(line_styles, marker_styles)\n",
    "    big_markers = ['o', 'big']\n",
    "    dash_styles = {'--': 'dash', ':': 'dot'}\n",
    "    \n",
    "    def substr_list(style, x):\n",
    "        '''\n",
    "        style: a style string\n",
    "        x: a list of substrings\n",
    "        \n",
    "        return: true if any of the strings in x is a substring of s, and false othewise    \n",
    "        '''\n",
    "        inds = np.array([s in style for s in x])\n",
    "        if np.any(inds):\n",
    "            return x[np.where(inds)[0][0]]\n",
    "        else:\n",
    "            return ''\n",
    "    \n",
    "    is_line = lambda s: substr_list(s, line_styles + combo_styles)\n",
    "    is_marker = lambda s: substr_list(s, marker_styles + combo_styles)\n",
    "    is_combo = lambda s: substr_list(s, combo_styles)\n",
    "    \n",
    "    is_dashed = lambda s: substr_list(s, list(dash_styles.keys()))\n",
    "    is_bigmarker = lambda s: substr_list(s, big_markers)\n",
    "    \n",
    "    reduced_data = batch_reducer(data, force_pandas=True, stack=True, **reduce_kwargs)\n",
    "    split_data = [x[1].set_index(x[1].index.get_level_values(1)) for x in list(reduced_data.groupby('ID'))]\n",
    "    \n",
    "    if reduced_data.shape[1] == 2:\n",
    "        suffix = ''\n",
    "    elif reduced_data.shape[1] == 3:\n",
    "        suffix = '3D'\n",
    "    else:\n",
    "        raise NotImplementedError(f'Unsupported number of dimensions; reduced data must be 2d or 3d, given: {reduced_data.shape[1]}')\n",
    "        \n",
    "    \n",
    "    \n",
    "    #TODO: add support for multiple styles (up to one per line)\n",
    "    #TODO: load in custom plot theme/style from defaults, also allow user to customize\n",
    "    #TODO: draw bounding box around space\n",
    "    #TODO: support animation:\n",
    "    # - camera rotation\n",
    "    # - window length around current timepoint\n",
    "    # - opacity of prior timepoints\n",
    "    # - opacity of current window\n",
    "    # - opacity of future timepoints    \n",
    "    #TODO: legend customization\n",
    "    #TODO: label each point with its original values and/or a user-specified label\n",
    "    #TODO: support per-datapoint colors, including for lines\n",
    "    #TODO: support for mixture-based colors-- if 1d, map onto colormap.  If 1 < n-colors <= 3, pick\n",
    "    #      a color for each dimension and then compute each coordinate as a weighted blend.  If > 3,\n",
    "    #      map onto 3D colors and then treat as 3D.  Can also define custom functions-- should take\n",
    "    #      in any observation and return a new vector describing its color (in >= 1D).  also support\n",
    "    #      a list of functions (one per data matrix)\n",
    "    #TODO: support plotting of error bars via streamtubes and/or ribbon plots\n",
    "    #TODO: support plotting \"vector fields\" by taking a list of two matrices of coordinats,\n",
    "    #      one specifying the start points and the other specifying the end points of each\n",
    "    #      vector\n",
    "    \n",
    "    #Write a general \"hypertools function\" that does all analyses by calling normalize --> align --> reduce\n",
    "    #in sequence.\n",
    "    \n",
    "    #TODO: support for nested (>2 level dataframes)\n",
    "    #TODO: support interpolation for line_styles and combo_styles\n",
    "    #TODO: define default options outside of this function in a dictionary\n",
    "    #TODO: normalization options: z-score down rows, across columns, both, or arbitrary functions (take\n",
    "    #      in a data matrix and return a new matrix of the same type/size).  can also provide lists of\n",
    "    #      normalization functions to be applied separately to each data matrix.\n",
    "    #TODO: add support for multiple reduce functions (and/or reduce_kwargs)-- one per data matrix\n",
    "    #TODO: add hyperalignment and SRM.  also allow different subsets of the data to be aligned, e.g.\n",
    "    #      according to the unique IDs specified in a list of group IDs, some column of the data matrix,\n",
    "    #      cluster IDs, or a custom function (or a dictionary of functions specifying how different\n",
    "    #      group IDs should be aligned); functions should take in a list of data matrices and returned an aligned\n",
    "    #      list in the same format.\n",
    "    \n",
    "    style = style.lower()\n",
    "    \n",
    "    assert style in marker_styles + line_styles + combo_styles, f'Unsupported plot style: {style}'\n",
    "    \n",
    "    if is_combo(style):\n",
    "        mode='lines+markers'\n",
    "    elif is_line(style):\n",
    "        mode='lines'\n",
    "    else:\n",
    "        mode='markers'\n",
    "    \n",
    "    if is_line(style):\n",
    "        if not('line' in kwargs.keys()):\n",
    "                kwargs['line'] = {}\n",
    "        if not('opacity' in kwargs['line'].keys()):\n",
    "            kwargs['line']['opacity'] = float(defaults['plot']['opacity'])\n",
    "        \n",
    "        dash = is_dashed(style)\n",
    "        if dash:\n",
    "            if not('dash' in kwargs['line'].keys()):\n",
    "                kwargs['line']['dash'] = dash\n",
    "    if is_marker(style):\n",
    "        if is_bigmarker(style):\n",
    "            size = int(defaults['plot']['bigmarkersize'])\n",
    "        else:\n",
    "            size = int(defaults['plot']['markersize'])\n",
    "        \n",
    "        if not('marker' in kwargs.keys()):\n",
    "            kwargs['marker'] = {}\n",
    "        if not('size' in kwargs['marker'].keys()):\n",
    "            if is_bigmarker(style):\n",
    "                kwargs['marker']['size'] = int(defaults['plot']['bigmarkersize'])\n",
    "            else:\n",
    "                kwargs['marker']['size'] = int(defaults['plot']['markersize'])        \n",
    "        if not('opacity' in kwargs['marker'].keys()):\n",
    "            kwargs['marker']['opacity'] = float(defaults['plot']['opacity'])            \n",
    "    \n",
    "    names = [str(n) for n in np.array(reduced_data.index.levels[0])]\n",
    "    \n",
    "    if reduced_data.shape[1] == 2:\n",
    "        shapes = [go.Scatter(x=d.iloc[:, 0], y=d.iloc[:, 1], mode=mode, name=names[i], **kwargs) for i, d in enumerate(split_data)]\n",
    "    elif reduced_data.shape[1] == 3:\n",
    "        shapes = [go.Scatter3d(x=d.iloc[:, 0], y=d.iloc[:, 1], z=d.iloc[:, 2], mode=mode, name=names[i], **kwargs) for i, d in enumerate(split_data)]\n",
    "    \n",
    "    return iplot(shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [np.cumsum(np.random.randn(100, 10), axis=0) for i in np.arange(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_r = batch_reducer(xs, algorithm='UMAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(xs, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(xs, style='--o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(xs, style='-o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(xs, style='.', reduce_kwargs={'ndims': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(xs, style='-', reduce_kwargs={'ndims': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://plot.ly/python/line-charts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [str(s) for s in np.arange(len(x_r))]\n",
    "data = [go.Scatter(x=x_r[i][:, 0], y=x_r[i][:, 1], mode='markers', name=n) for i,n in enumerate(names)]\n",
    "iplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [str(s) for s in np.arange(len(x_r))]\n",
    "data = [go.Scatter(x=x_r[i][:, 0], y=x_r[i][:, 1], mode='lines', name=n) for i,n in enumerate(names)]\n",
    "iplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [str(s) for s in np.arange(len(x_r))]\n",
    "data = [go.Scatter3d(x=x_r[i][:, 0], y=x_r[i][:, 1], z=x_r[i][:, 2], mode='markers', name=n) for i,n in enumerate(names)]\n",
    "iplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [str(s) for s in np.arange(len(x_r))]\n",
    "data = [go.Scatter3d(x=x_r[i][:, 0], y=x_r[i][:, 1], z=x_r[i][:, 2], mode='lines', name=n) for i,n in enumerate(names)]\n",
    "iplot(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
