{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import modin.pandas as pd\n",
    "import pandas as pd\n",
    "from ppca import PPCA\n",
    "import six\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, AgglomerativeClustering, Birch, FeatureAgglomeration, SpectralClustering, SpectralCoclustering, SpectralBiclustering, DBSCAN, AffinityPropagation, MeanShift\n",
    "from sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n",
    "from sklearn.decomposition import PCA, FastICA, IncrementalPCA, KernelPCA, FactorAnalysis, TruncatedSVD, SparsePCA, MiniBatchSparsePCA, DictionaryLearning, MiniBatchDictionaryLearning\n",
    "from sklearn.manifold import TSNE, MDS, SpectralEmbedding, LocallyLinearEmbedding, Isomap\n",
    "from umap import UMAP\n",
    "from flair.embeddings import WordEmbeddings, CharacterEmbeddings, StackedEmbeddings, FlairEmbeddings, BertEmbeddings, ELMoEmbeddings, DocumentPoolEmbeddings\n",
    "from flair.data import Sentence\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "global HYPERTOOLS_NOTEBOOK_MODE; HYPERTOOLS_NOTEBOOK_MODE = False\n",
    "\n",
    "#import plotly_express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugging stuff\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypertools_notebook_mode(toggle=None):\n",
    "    global HYPERTOOLS_NOTEBOOK_MODE\n",
    "    \n",
    "    if toggle:\n",
    "        print('Enabling inline hypertools plots')\n",
    "        HYPERTOOLS_NOTEBOOK_MODE = True\n",
    "    else:\n",
    "        print('Disabling interactive hypertools plots')\n",
    "        HYPERTOOLS_NOTEBOOK_MODE = False\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling inline hypertools plots\n"
     ]
    }
   ],
   "source": [
    "hypertools_notebook_mode(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-28 16:27:04,023 From /usr/local/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:63: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import tensorflow.compat.v1 as tf\n",
    "#import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srm import SRM\n",
    "from procrustes import procrustes\n",
    "\n",
    "#working from align.py:\n",
    "# - include SRM, hyperalignment, and procrustes as options (for procrustes, align everything to the first observation)\n",
    "#   - modify these functions to return both the transformed data and also a function that will map a new set of observations into the common space\n",
    "# - format the data to ensure it's a list of dataframes\n",
    "# - carve out hyperalignment as a separate function (like SRM)\n",
    "# - use the indices to create a common set of dataframes, all with indices from the full set (union) of indices across all dataframes in the list, and all with the same number of columns (zero-padding to match the dataframe with the most columns).  compute the transformations\n",
    "#   using just the common indices (intersection).  But the transform function should work with arbitrary data, including from non-matching indices.\n",
    "#   The full align function should return a list of dataframes of the same size/shape as the inputted list, and where each dataframe has the same number of rows (at the same indices)\n",
    "#   as the corresponding dataframes in the original list, but every dataframe in the aligned list has the same number of columns.\n",
    "\n",
    "#consider: returnning a new sub-class of dataframe that includes the inverse transformation functions, allowing the data to be transformed back into the original data spaces\n",
    "#in turn, this would return a new TransformableDataFrame that could map the (original) data back into the transformed space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "#from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults = ConfigParser()\n",
    "defaults.read('defaults.ini');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_models = ['DictionaryLearning', 'FactorAnalysis', 'FastICA', 'IncrementalPCA', 'KernelPCA', 'LatentDirichletAllocation', 'MiniBatchDictionaryLearning',\n",
    " 'MiniBatchSparsePCA', 'NMF', 'PCA', 'SparseCoder', 'SparsePCA', 'TruncatedSVD', 'UMAP', 'TSNE', 'MDS', 'SpectralEmbedding', 'LocallyLinearEmbedding', 'Isomap']\n",
    "cluster_models = ['AffinityPropagation', 'AgglomerativeClustering', 'Birch', 'DBSCAN', 'FeatureAgglomeration', 'KMeans', 'MeanShift', 'MiniBatchKMeans', 'SpectralBiclustering', 'SpectralClustering', 'SpectralCoclustering', 'DBSCAN', 'AffinityPropagation', 'MeanShift']\n",
    "mixture_models = ['GaussianMixture', 'BayesianGaussianMixture', 'LatentDirichletAllocation', 'NMF']\n",
    "decomposition_models = ['LatentDirichletAllocation', 'NMF']\n",
    "text_vectorizers = ['CountVectorizer', 'TfidfVectorizer']\n",
    "interpolation_models = ['linear', 'time', 'index', 'pad', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'spline', 'barycentric', 'polynomial']\n",
    "text_models = ['universal_sentence_encoder', 'LatentDirichletAllocation', 'NMF']\n",
    "corpora = ['wiki', 'nips', 'sotus']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API ideas\n",
    "\n",
    "1. [x] Write a single function to format data into a list of pandas dataframes:\n",
    "    - [x] If text, should vectorize text according to default (or given) models.  Allow user to (optionally) pass in row/column labels for the text data.  Each string gets one row; a list of strings yields a len(list) by n-features dataframe\n",
    "    - [x] If numpy arrays, set indices to row count and columns to column count\n",
    "    - [x] If pandas dataframe, keep as is but turn into a list\n",
    "    - [x] If only a single matrix is given, still return a list\n",
    "    - [x] Dimensions may be mismatched\n",
    "    - [x] Missing data is left unchanged\n",
    "2. [x] Write a function to take a list of dataframes and use PPCA to fill in missing values.  Also use interpolation to fill in missing rows\n",
    "3. [x] Write a reduce function that can work in batch mode (reduce a single dataframe or a list of dataframes)\n",
    "4. [x] Write a cluster function that can work in batch mode (return cluster labels or mixture proportions for a single dataframe or a list of dataframes)\n",
    "5. Write a text-vectorizer function that wraps universal sentence encoder, LDA, NMF, etc.\n",
    "6. Write an align function that aligns all dataframes in a list into a common space\n",
    "7. Write a normalize function that z-scores the data\n",
    "8. Write a plot function that takes a list of dataframes and plots them\n",
    "    - Need a mat2colors function that colorizes data points according to group labels, cluster assignments, mixture proportions, or user-specified matrices.  Take in a colormap as an argument\n",
    "    - Need support for multicolored lines in addition to per-observation marker colors.  For 3d plots, could use streamtubes...but for 2d plots this won't work, so maybe a segment solution would be appropriate.\n",
    "    - Potentially allow for different line/marker colors\n",
    "    - Support for animations:\n",
    "      - Spin\n",
    "      - Sliding window\n",
    "      - Sliding window with tail\n",
    "      - Sliding window with head\n",
    "    - Support for multiindex dataframes (hierarchical plots with different coloring/line thickness)\n",
    "9. Write a hyper-function that takes in a \"messy\" dataset, applies formatting, PPCA, reduce, alignment, and clustering (as specified) and returns a formatted/processed dataset.\n",
    "10. Add hyper-function call to every helper function, so that all analyses can be applied from any call to any other function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dataframe(x):\n",
    "    return type(x).__module__ in ['pandas.core.frame', 'modin.pandas.dataframe']\n",
    "\n",
    "def is_multiindex_dataframe(x):\n",
    "    return is_dataframe(x) and (type(x.index).__module__ == 'pandas.core.indexes.multi')\n",
    "\n",
    "def is_array(x):\n",
    "    return (not ('str' in str(type(x)))) and (type(x).__module__ == 'numpy')\n",
    "\n",
    "def is_empty(x):\n",
    "    return (x is None) or (len(x) == 0)\n",
    "\n",
    "def is_text(x):\n",
    "    if type(x) == list:\n",
    "        return np.all([is_text(t) for t in x])\n",
    "    return (type(x) in six.string_types) or (type(x) == np.str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_str_list(x, encoding='utf-8'):\n",
    "    def to_string(s):\n",
    "        if type(s) == str:\n",
    "            return s\n",
    "        elif is_empty(s) or (s == None):\n",
    "            return ''\n",
    "        elif type(s) in [bytes, np.bytes_]:\n",
    "            return s.decode(encoding)\n",
    "        elif is_array(s) or is_dataframe(s) or (type(s) == list):\n",
    "            if len(s) == 1:\n",
    "                return to_string(s[0])\n",
    "            else:\n",
    "                return to_str_list(s, encoding=encoding)\n",
    "        else:\n",
    "            return str(s)\n",
    "    \n",
    "    if is_array(x) or (type(x) == list):        \n",
    "        return [to_string(s) for s in x]\n",
    "    elif is_text(x):\n",
    "        return [x]\n",
    "    else:\n",
    "        raise Exception('Unsupported data type: {type(x)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(c, encoding='utf-8'):\n",
    "    import hypertools as hyp\n",
    "    if c in corpora:\n",
    "        fname = os.path.join(eval(defaults['data']['datadir']),'corpora', f'{c}.npy')\n",
    "        if not os.path.exists(fname):\n",
    "            if not os.path.exists(os.path.abspath(os.path.join(fname, os.pardir))):\n",
    "                os.makedirs(os.path.abspath(os.path.join(fname, os.pardir)))\n",
    "            corpus_words = to_str_list(hyp.load(c).data[0]) #TODO: FIX THIS TO NOT CALL HYPERTOOLS!\n",
    "            \n",
    "            np.save(fname, corpus_words)\n",
    "            return corpus_words\n",
    "        else:\n",
    "            corpus_words = np.load(fname, allow_pickle=True)\n",
    "            return to_str_list(corpus_words)\n",
    "    else:\n",
    "        if is_text(c):\n",
    "            if type(c) == list:\n",
    "                return c\n",
    "            else:\n",
    "                return [c]\n",
    "        elif os.path.exists(c):\n",
    "            return to_str_list([x[0] for x in np.load(c, allow_pickle=True).tolist()])\n",
    "        else:\n",
    "            raise Exception(f'Unknown corpus: {c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, vectorizer='CountVectorizer', vocabulary=None, return_vocab=False):\n",
    "    if not (type(text) == list):\n",
    "        text = [text]\n",
    "    assert is_text(text), f'Must vectorize a string or list of strings (given: {type(text)})'\n",
    "    \n",
    "    if type(vectorizer) in six.string_types:\n",
    "        assert vectorizer in text_vectorizers, f'Text vectorizer must be a function or a member of {text_vectorizers}'\n",
    "        vectorizer = eval(vectorizer)\n",
    "    assert callable(vectorizer), f'Text vectorizer must be a function or a member of {text_vectorizers}'\n",
    "    \n",
    "    text2vec = vectorizer(max_df=eval(defaults['text']['max_df']),\n",
    "                          min_df=eval(defaults['text']['min_df']),\n",
    "                          stop_words=defaults['text']['stop_words'],\n",
    "                          strip_accents=defaults['text']['strip_accents'],\n",
    "                          lowercase=eval(defaults['text']['lowercase']),\n",
    "                          vocabulary=vocabulary)\n",
    "    vectorized_text = text2vec.fit_transform(text)\n",
    "    \n",
    "    if not return_vocab:\n",
    "        return vectorized_text\n",
    "    else:\n",
    "        vocab = text2vec.get_feature_names()\n",
    "        return vectorized_text, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_model(corpus, model, vectorizer, n_components=50):\n",
    "    if type(model) in six.string_types:\n",
    "        assert model in text_models, f'Text model must be a function or a member of {text_models}'\n",
    "        model = eval(model)\n",
    "    assert callable(model), f'Text model must be a function or a member of {text_models}'\n",
    "    \n",
    "    if type(vectorizer) in six.string_types:\n",
    "        assert vectorizer in text_vectorizers, f'Text vectorizer must be a function or a member of {text_vectorizers}'\n",
    "        vectorizer = eval(vectorizer)\n",
    "    assert callable(vectorizer), f'Text vectorizer must be a function or a member of {text_vectorizers}'\n",
    "    \n",
    "    if corpus in corpora:\n",
    "        saveable = True\n",
    "    else:\n",
    "        if not os.path.exists(corpus):        \n",
    "            assert is_text(corpus), f'Corpus must be a list of strings, or one of {corpora}'\n",
    "        saveable = False\n",
    "    \n",
    "    if saveable:\n",
    "        fname = os.path.join(eval(defaults['data']['datadir']),'text-models', model.__name__, f'{corpus}-{vectorizer.__name__}-{n_components}.npz')    \n",
    "        if not os.path.exists(os.path.abspath(os.path.join(fname, os.pardir))):\n",
    "            os.makedirs(os.path.abspath(os.path.join(fname, os.pardir)))\n",
    "    \n",
    "    if saveable and os.path.exists(fname):\n",
    "        with np.load(fname, allow_pickle=True) as x:\n",
    "            return {'vocab': x['vocab'].tolist(), 'model': x['model'].tolist()}\n",
    "    else:\n",
    "        corpus = get_corpus(corpus)\n",
    "        vectorized_corpus, vocab = vectorize_text(corpus, vectorizer=vectorizer, return_vocab=True)\n",
    "        \n",
    "        if n_components == None:\n",
    "            n_components = eval(defaults['text']['topics'])\n",
    "        args = {'n_components': n_components,\n",
    "                'max_iter': eval(defaults['text']['max_iter'])}\n",
    "        \n",
    "        if model.__name__ == 'NMF' and (args['n_components'] > len(corpus)):\n",
    "            args['n_components'] = len(corpus)\n",
    "        \n",
    "        if model.__name__ == 'LatentDirichletAllocation':\n",
    "            args['learning_method'] = defaults['text']['learning_method']\n",
    "            args['learning_offset'] = eval(defaults['text']['learning_offset'])\n",
    "        \n",
    "        #return args, vectorized_corpus, vocab\n",
    "        \n",
    "        embeddings = model(**args).fit(vectorized_corpus)\n",
    "        \n",
    "        if saveable:\n",
    "            np.savez(fname, vocab=vocab, model=embeddings)\n",
    "        \n",
    "        return {'vocab': vocab, 'model': embeddings}        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_NMF_model = get_text_model('wiki', 'NMF', 'CountVectorizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_LDA_model = get_text_model('wiki', 'LatentDirichletAllocation', 'CountVectorizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "nips_NMF_model = get_text_model('nips', 'NMF', 'CountVectorizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "nips_LDA_model = get_text_model('nips', 'LatentDirichletAllocation', 'CountVectorizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotus_NMF_model = get_text_model('sotus', 'NMF', 'CountVectorizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotus_LDA_model = get_text_model('sotus', 'LatentDirichletAllocation', 'CountVectorizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_vectorizer(text, model='universal_sentence_encoder', **kwargs):\n",
    "    def universal_sentence_encoder(text, **kwargs):        \n",
    "        if 'USE_corpus' in kwargs.keys():\n",
    "            corpus = kwargs['USE_corpus']\n",
    "        else:\n",
    "            corpus = defaults['text']['USE_corpus']\n",
    "            #if not os.path.exists(corpus):\n",
    "            #    os.system(f\"'https://tfhub.dev/google/universal-sentence-encoder-large/3?tf-hub-format=compressed' | tar -zxvC {defaults['text']['USE_corpus']}\")\n",
    "        \n",
    "        assert os.path.exists(corpus), f'Corpus not found: {corpus}'\n",
    "\n",
    "        encoder = hub.Module(corpus)\n",
    "\n",
    "        tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "        with tf.Session() as session:\n",
    "            session.run([tf.global_variables_initializer(), tf.tables_initializer()])        \n",
    "            return session.run(encoder(text))\n",
    "    \n",
    "    def sklearn_vectorizer(text, model, **kwargs):                \n",
    "        if 'corpus' in kwargs.keys():\n",
    "            corpus = kwargs['corpus']\n",
    "        else:\n",
    "            corpus = defaults['text']['corpus']            \n",
    "        \n",
    "        assert (corpus in corpora) or is_text(corpus) or os.path.exists(corpus), f'Cannot use corpus: {corpus}'\n",
    "        \n",
    "        if 'vectorizer' in kwargs.keys():\n",
    "            vecterizer = kwargs['vectorizer']\n",
    "            kwargs.pop('vectorizer', None)\n",
    "        else:\n",
    "            vectorizer = defaults['text']['vectorizer']\n",
    "        \n",
    "        model = get_text_model(corpus, model, vectorizer)\n",
    "        return model['model'].transform(vectorize_text(text, vectorizer=vectorizer, vocabulary=model['vocab']))\n",
    "    \n",
    "    assert (model in text_models) or (callable(model)), f'Unsupported model: {model}'\n",
    "    if not (type(text) == list):\n",
    "        text = [text]\n",
    "    \n",
    "    if callable(model):\n",
    "        return model(text, **kwargs)\n",
    "    elif model == 'universal_sentence_encoder':\n",
    "        return universal_sentence_encoder(text, **kwargs)\n",
    "    else:\n",
    "        return sklearn_vectorizer(text, model, **kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.10979288e-05, 0.00000000e+00,\n",
       "        3.06367804e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 5.89054549e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.27658891e-06, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 8.41868306e-06, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.38171181e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.46007816e-06, 0.00000000e+00,\n",
       "        1.41852061e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.88956442e-06, 5.51825141e-05, 0.00000000e+00,\n",
       "        2.37589758e-05, 0.00000000e+00, 0.00000000e+00, 5.24241585e-05,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.98558929e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.31221454e-04, 6.58254467e-05,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.38971378e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.76769334e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer(['happy birthday', 'cat dog is a happy cat'], 'NMF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(vectorized_corpus.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(vectorized_corpus.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-23ce9c511939>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wiki'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-c83909419c95>\u001b[0m in \u001b[0;36mget_corpus\u001b[0;34m(c, encoding)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#TODO: THIS STATEMENT IS BROKEN...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbytes_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                     \u001b[0mcorpus_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Unuseable format: {type(corpus_words[0][0])}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-c83909419c95>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#TODO: THIS STATEMENT IS BROKEN...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbytes_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                     \u001b[0mcorpus_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Unuseable format: {type(corpus_words[0][0])}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "corpus = get_corpus('wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = CountVectorizer(stop_words='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweet_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-bb75b344bc09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtf_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweet_text' is not defined"
     ]
    }
   ],
   "source": [
    "n_features = 1000\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(tweet_text)    \n",
    "vocab = tf_vectorizer.get_feature_names()\n",
    "\n",
    "if model == None:\n",
    "    lda = LDA(n_topics=n_topics, max_iter=5, learning_method='online', learning_offset=50., random_state=0).fit(tf)\n",
    "else:\n",
    "    lda = model            \n",
    "tweet_topics = lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def panda_stack(data, force=True, names=None, keys=None, verify_integrity=False, sort=False, copy=True, ignore_index=False, levels=None, **kwargs):\n",
    "    '''\n",
    "    Take a list of DataFrames with the same number of columns and (optionally)\n",
    "    a list of names (of the same length as the original list; default:\n",
    "    range(len(x))).  Return a single MultiIndex DataFrame where the original\n",
    "    DataFrames are stacked vertically, with the data names as their level 1\n",
    "    indices and their original indices as their level 2 indices.\n",
    "    \n",
    "    INPUTS\n",
    "    data: data in any format (text, numpy arrays, pandas dataframes, or a mixed list (or nested lists) of those types)\n",
    "    text_vectorizer: function that takes a string (or list of strings) and returns a numpy array or dataframe.  If\n",
    "    force is False, must pass in a list of DataFrames.\n",
    "    \n",
    "    force: if True, use format_data to coerce everything into a list of pandas dataframes.\n",
    "    \n",
    "    text_vectorizer: function for turning text data into DataFrames, used if force is True\n",
    "    \n",
    "    Also takes all keyword arguments from pandas.concat except axis, join, join_axes\n",
    "    \n",
    "    All other keywork arguments (if any) are passed to text_vectorizer\n",
    "    \n",
    "    OUTPUTS\n",
    "    a single MultiIndex DataFrame\n",
    "    '''\n",
    "    \n",
    "    if force:\n",
    "        data = format_data(data, **kwargs)\n",
    "    \n",
    "    assert type(data) == list, 'Must either pass a list of DataFrames or set force to True'\n",
    "    assert np.all([is_dataframe(d) for d in data]), 'Must either pass a list of DataFrames or set force to True'\n",
    "    assert len(np.unique([d.shape[1] for d in data])) == 1, 'All DataFrames must have the same number of columns'    \n",
    "    template = data[0].columns.values\n",
    "    for i, d1 in enumerate(data):\n",
    "        template = d1.columns.values\n",
    "        for d2 in data[(i+1):]:\n",
    "            assert np.all([(c in template) for c in d2.columns.values]), 'All DataFrames must have the same columns'\n",
    "    \n",
    "    if keys is None:\n",
    "        keys = np.arange(len(data), dtype=np.int)\n",
    "    \n",
    "    assert is_array(keys) or (type(keys) == list), f'keys must be None or a list or array of length len(data)'\n",
    "    assert len(keys) == len(data), f'keys must be None or a list or array of length len(data)'\n",
    "    \n",
    "    return pd.concat(data, axis=0, join='outer', join_axes=None, names=names, keys=keys, \n",
    "                     verify_integrity=verify_integrity, sort=sort, copy=copy, ignore_index=ignore_index, levels=levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def panda_unstack(x):\n",
    "    if not is_multiindex_dataframe(x):\n",
    "        if is_dataframe(x):\n",
    "            return x\n",
    "        else:\n",
    "            raise Exception(f'Unsupported datatype: {type(x)}')\n",
    "    \n",
    "    names = list(x.index.names)\n",
    "    grouper = 'ID'\n",
    "    if not (grouper in names):\n",
    "        names[0] = grouper\n",
    "    elif not (names[0] == grouper):\n",
    "        for i in np.arange(len(names)): #trying n things other than 'ID'; at least one of them must be outside of the n-1 remaining names\n",
    "            next_grouper = f'{grouper}{i}'\n",
    "            if not (next_grouper in names):\n",
    "                names[0] = next_grouper\n",
    "                grouper = next_grouper\n",
    "                break\n",
    "    assert names[0] == grouper, 'Unstacking error'\n",
    "    \n",
    "    x.index.rename(names, inplace=True)    \n",
    "    unstacked = [d[1].set_index(d[1].index.get_level_values(1)) for d in list(x.groupby(grouper))]\n",
    "    if len(unstacked) == 1:\n",
    "        return unstacked[0]\n",
    "    else:\n",
    "        return unstacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(x, **kwargs):\n",
    "    '''\n",
    "    INPUTS\n",
    "    x: data in any format (text, numpy arrays, pandas dataframes, or a mixed list (or nested lists) of those types)\n",
    "    text_vectorizer: function that takes a string (or list of strings) and returns a numpy array or dataframe\n",
    "    text_kwargs: dictionary of keywork arguments to pass to text_vectorizer\n",
    "    \n",
    "    OUTPUTS\n",
    "    a list of pandas dataframes\n",
    "    '''\n",
    "    \n",
    "    def to_dataframe(y):\n",
    "        if is_dataframe(y):\n",
    "            return y\n",
    "        elif is_array(y):\n",
    "            assert np.ndim(y) == 2, 'all data matrices must be 2-dimensional'\n",
    "            idx = np.arange(y.shape[0])\n",
    "            cols = np.arange(y.shape[1])\n",
    "            return pd.DataFrame(data=y, index=idx, columns=cols)\n",
    "        elif type(y) == list:\n",
    "            return [to_dataframe(i) for i in y]\n",
    "        elif is_text(y):\n",
    "            if 'text_model' in kwargs.keys():\n",
    "                text_model = kwargs['text_model']\n",
    "            else:\n",
    "                text_model = defaults['text']['model']\n",
    "                \n",
    "            if 'text_kwargs' in kwargs.keys():\n",
    "                text_kwargs = kwargs['text_kwargs']\n",
    "            else:\n",
    "                text_kwargs = {}\n",
    "            return to_dataframe(text_vectorizer(y, model=text_model, **text_kwargs))\n",
    "        else:\n",
    "            raise NotImplementedError(f'Unsupported data type: {type(y)}')\n",
    "    \n",
    "    if type(x) == list:\n",
    "        return [to_dataframe(i) for i in x]\n",
    "    else:\n",
    "        return [to_dataframe(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing(x, apply_ppca=True, interpolation='linear', **kwargs):\n",
    "    if type(x) == list:\n",
    "        return [fill_missing(i, apply_ppca=apply_ppca, interpolation=interpolation, **kwargs) for i in x]\n",
    "    elif is_array(x):\n",
    "        return fill_missing(pd.DataFrame(x), apply_ppca=apply_ppca, interpolation=interpolation, **kwargs).values\n",
    "    elif not is_dataframe(x):\n",
    "        raise Exception('Unsupported datatype: f{type(x)}')\n",
    "    \n",
    "    if 'method' in interpolate_kwargs.keys():\n",
    "        warnings.warn(f\"Overloaded keyword argument; ignoring interpolation parameter ({interpolation}) in favor of keyword-specified method: {interpolate_kwargs['method']}\")\n",
    "    else:\n",
    "        interpolate_kwargs['method'] = interpolation\n",
    "    \n",
    "    assert interpolate_kwargs['method'] in interpolation_models, f\"Unsupported interpolation type: '{interpolate_kwargs['method']}'\"\n",
    "    \n",
    "    if apply_ppca:\n",
    "        covariance_model = PPCA()\n",
    "        covariance_model.fit(x.values)\n",
    "        x.values = covariance_model.transform()\n",
    "    \n",
    "    if not ('inplace' in kwargs.keys()):\n",
    "        kwargs['inplace'] = False\n",
    "    \n",
    "    if not ('axis' in kwargs.keys()):\n",
    "        kwargs['axis'] = 0\n",
    "        \n",
    "    if 'method' in kwargs.keys():\n",
    "        warnings.warn(f\"Overloaded keyword argument; ignoring interpolation parameter ({interpolation}) in favor of keyword-specified method: {kwargs['method']}\")\n",
    "    else:\n",
    "        kwargs['method'] = interpolation\n",
    "    \n",
    "    return x.interpolate(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(data, algorithm='IncrementalPCA', stack=False, keys=None, ndims=3, fillna=False, interpolation_kwargs={}, **kwargs):\n",
    "    '''\n",
    "    ARGUMENTS:\n",
    "    data: data to reduce (numpy array or compatable, or a pandas\n",
    "          dataframe or compatable).  Formatted as a 2d matrix whose\n",
    "          rows are observations and whose columns are feature\n",
    "          dimensions.\n",
    "    \n",
    "    algorithm: one of: 'DictionaryLearning', 'FactorAnalysis',\n",
    "          'FastICA', 'IncrementalPCA', 'KernelPCA',\n",
    "          'LatentDirichletAllocation', 'MiniBatchDictionaryLearning',\n",
    "          'MiniBatchSparsePCA', 'NMF', 'PCA', 'SparseCoder',\n",
    "          'SparsePCA', 'TruncatedSVD', or 'UMAP'.  Calls the\n",
    "          relevant scikit-learn or UMAP function.  Can also\n",
    "          pass a function directly.\n",
    "    \n",
    "    ndims: the number of dimensions (columns) in the result (default: 3).\n",
    "          if ndims > data.shape[1], the right-most columns will be\n",
    "          zero-padded.\n",
    "    \n",
    "    fillna: if True, use PPCA and interpolation to fill in nan-valued entries (default: False)\n",
    "    \n",
    "    interpolation_kwargs: keyword arguments passed to the interpolation function, used\n",
    "           when fillna is True.\n",
    "    \n",
    "    stack: if True, create a single (stacked) MultiIndex DataFrame out of\n",
    "           the inputted data list and return a version of the same dataframe,\n",
    "           but with ndims columns (named range(ndims))\n",
    "    \n",
    "    keys: a name for each data matrix (default: None; name each set of observations\n",
    "          range(len(data))).  Only relevant when stack is True.\n",
    "    \n",
    "    all additional keyword arguments are passed to the reduce function\n",
    "    \n",
    "    RETURNS:\n",
    "    pandas dataframe (or list of dataframes) with number-of-observations rows and\n",
    "    ndims columns\n",
    "    '''\n",
    "    \n",
    "    if type(algorithm) == str:\n",
    "        assert algorithm in reduce_models, f'Unknown algorithm: {algorithm}' #security check to prevent executing arbitrary code\n",
    "        algorithm = eval(algorithm)\n",
    "    else:\n",
    "        if not (('sklearn.decomposition' in algorithm.__module__) or ('umap.umap_' in algorithm.__module__)):\n",
    "            raise Exception(f'Unknown algorithm: {algorithm}')\n",
    "    \n",
    "    assert ndims >= 0, f'Number of dimensions must be non-negative (given: {ndims})'\n",
    "    \n",
    "    warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "    \n",
    "    data = format_data(data)\n",
    "    \n",
    "    if fillna:\n",
    "        data = fill_missing(data, **interpolation_kwargs)\n",
    "    \n",
    "    stacked_data = panda_stack(data, keys=keys)\n",
    "    vals = stacked_data.values\n",
    "    \n",
    "    if vals.shape[1] > ndims:\n",
    "        model = algorithm(n_components=ndims, **kwargs)\n",
    "        reduced_vals = model.fit_transform(vals)\n",
    "    elif vals.shape[1] < ndims:\n",
    "        reduced_vals = np.concatenate([vals, np.zeros([vals.shape[0], ndims-vals.shape[1]])], axis=1)\n",
    "    else:\n",
    "        reduced_vals = vals\n",
    "    \n",
    "    reduced_data = pd.DataFrame(data=reduced_vals, index=stacked_data.index, columns=np.arange(ndims))\n",
    "    if stack:\n",
    "        return reduced_data\n",
    "    else:\n",
    "        return panda_unstack(reduced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(data, algorithm='KMeans', k=10, stack=False, keys=None, fillna=False, interpolation_kwargs={}, return_model=False, **kwargs):\n",
    "    if type(algorithm) == str:\n",
    "        assert (algorithm in cluster_models) or (algorithm in mixture_models), f'Unknown algorithm: {algorithm}' #security check to prevent executing arbitrary code\n",
    "        algorithm = eval(algorithm)\n",
    "    else:\n",
    "        if not (('sklearn.cluster' in algorithm.__module__) or ('sklearn.decomposition' in algorithm.__module__) or ('sklearn.mixture' in algorithm.__module__)):\n",
    "            raise Exception(f'Unknown algorithm: {algorithm}')\n",
    "    \n",
    "    k_dict = {'cluster_models': 'n_clusters', 'mixture_models': 'n_components'}\n",
    "    for model_type in k_dict.keys():\n",
    "        if algorithm.__name__ in eval(model_type):\n",
    "            if not (k_dict[model_type] in kwargs.keys()):\n",
    "                kwargs[k_dict[model_type]] = k\n",
    "            else:\n",
    "                warnings.warn(f\"Overloaded keyword argument; ignoring k value ({k}) in favor of keyword-specified {k_dict[model_type]}: {kwargs[k_dict[model_type]]}\")\n",
    "            break\n",
    "    \n",
    "    if not ('verbose' in kwargs.keys()):\n",
    "        kwargs['verbose'] = eval(defaults['cluster']['verbose'])\n",
    "    \n",
    "    data = format_data(data)\n",
    "    \n",
    "    if fillna:\n",
    "        data = fill_missing(data, **interpolation_kwargs)\n",
    "    \n",
    "    stacked_data = panda_stack(data, keys=keys)\n",
    "    vals = stacked_data.values\n",
    "    \n",
    "    if algorithm.__name__ in decomposition_models:\n",
    "        if np.min(vals) < 0:\n",
    "            vals -= np.min(vals) #avoid negative values by setting minimum value to 0\n",
    "    \n",
    "    model = algorithm(**kwargs)\n",
    "    model.fit(vals)\n",
    "    \n",
    "    if hasattr(model, 'n_clusters'):\n",
    "        n = 1\n",
    "    elif hasattr(model, 'n_components'):\n",
    "        n = model.n_components\n",
    "    else:\n",
    "        raise NotImplementedError(f'Unsupported model: {model}')\n",
    "    \n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        labels = model.predict_proba(vals)\n",
    "    elif hasattr(model, 'predict'):\n",
    "        labels = model.predict(vals)\n",
    "    elif hasattr(model, 'transform'):\n",
    "        labels = model.transform(vals)\n",
    "    else:\n",
    "        raise NotImplementedError(f'Unsupported model: {model}')\n",
    "    \n",
    "    labels = np.array(labels, ndmin=2)\n",
    "    if n == 1:\n",
    "        labels = labels.T\n",
    "    \n",
    "    stacked_labels = pd.DataFrame(data=labels, index=stacked_data.index, columns=np.arange(n))\n",
    "    \n",
    "    if stack:\n",
    "        if return_model:\n",
    "            return stacked_labels, model\n",
    "        else:\n",
    "            return stacked_labels\n",
    "    else:\n",
    "        if return_model:\n",
    "            return panda_unstack(stacked_labels), model\n",
    "        else:\n",
    "            return panda_unstack(stacked_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [np.cumsum(np.random.randn(100, 10), axis=0) for i in np.arange(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_stacked = panda_stack(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster(xs, stack=True, algorithm='GaussianMixture', k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import plot as pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(data, style='line', reduce_kwargs={}, **kwargs):\n",
    "    global HYPERTOOLS_NOTEBOOK_MODE\n",
    "        \n",
    "    def combo_merge(a, b):\n",
    "        combos = []\n",
    "        for i in a:\n",
    "            for j in b:\n",
    "                if (len(i) <= 2):\n",
    "                    if (len(j) <= 2):\n",
    "                        combos.append(i+j)\n",
    "                elif len(j) > 1:\n",
    "                    combos.append(i + '+' + j)\n",
    "        return combos\n",
    "    \n",
    "    marker_styles = ['.', 'o', 'scatter', 'marker', 'markers', 'bigmarker', 'bigmarkers']\n",
    "    line_styles = ['-', '--', ':', '-:', 'line', 'lines']\n",
    "    combo_styles = combo_merge(marker_styles, line_styles) + combo_merge(line_styles, marker_styles)\n",
    "    big_markers = ['o', 'big']\n",
    "    dash_styles = {'--': 'dash', '-:': 'dashdot', ':': 'dot'}\n",
    "    \n",
    "    def substr_list(style, x):\n",
    "        '''\n",
    "        style: a style string\n",
    "        x: a list of substrings\n",
    "        \n",
    "        return: true if any of the strings in x is a substring of s, and false othewise    \n",
    "        '''\n",
    "        inds = np.array([s in style for s in x])\n",
    "        if np.any(inds):\n",
    "            return x[np.where(inds)[0][0]]\n",
    "        else:\n",
    "            return ''\n",
    "    \n",
    "    is_line = lambda s: substr_list(s, line_styles + combo_styles)\n",
    "    is_marker = lambda s: substr_list(s, marker_styles + combo_styles)\n",
    "    is_combo = lambda s: substr_list(s, combo_styles)\n",
    "    \n",
    "    is_dashed = lambda s: substr_list(s, list(dash_styles.keys()))\n",
    "    is_bigmarker = lambda s: substr_list(s, big_markers)\n",
    "    \n",
    "    reduced_data = reduce(data, stack=True, **reduce_kwargs)\n",
    "    split_data = panda_unstack(reduced_data)\n",
    "    \n",
    "    if reduced_data.shape[1] == 2:\n",
    "        suffix = ''\n",
    "    elif reduced_data.shape[1] == 3:\n",
    "        suffix = '3D'\n",
    "    else:\n",
    "        raise NotImplementedError(f'Unsupported number of dimensions; reduced data must be 2d or 3d, given: {reduced_data.shape[1]}')\n",
    "    \n",
    "    #TODO: add support for multiple styles (up to one per line)\n",
    "    #TODO: load in custom plot theme/style from defaults, also allow user to customize\n",
    "    #TODO: draw bounding box around space\n",
    "    #TODO: support animation:\n",
    "    # - camera rotation\n",
    "    # - window length around current timepoint\n",
    "    # - opacity of prior timepoints\n",
    "    # - opacity of current window\n",
    "    # - opacity of future timepoints    \n",
    "    #TODO: legend customization\n",
    "    #TODO: label each point with its original values and/or a user-specified label\n",
    "    #TODO: support per-datapoint colors, including for lines\n",
    "    #TODO: support for mixture-based colors-- if 1d, map onto colormap.  If 1 < n-colors <= 3, pick\n",
    "    #      a color for each dimension and then compute each coordinate as a weighted blend.  If > 3,\n",
    "    #      map onto 3D colors and then treat as 3D.  Can also define custom functions-- should take\n",
    "    #      in any observation and return a new vector describing its color (in >= 1D).  also support\n",
    "    #      a list of functions (one per data matrix)\n",
    "    #TODO: support plotting of error bars via streamtubes and/or ribbon plots\n",
    "    #TODO: support plotting \"vector fields\" by taking a list of two matrices of coordinats,\n",
    "    #      one specifying the start points and the other specifying the end points of each\n",
    "    #      vector\n",
    "    \n",
    "    #Write a general \"hypertools function\" that does all analyses by calling normalize --> align --> reduce\n",
    "    #in sequence.\n",
    "    \n",
    "    #TODO: support for nested (>2 level dataframes)\n",
    "    #TODO: support interpolation for line_styles and combo_styles\n",
    "    #TODO: define default options outside of this function in a dictionary\n",
    "    #TODO: normalization options: z-score down rows, across columns, both, or arbitrary functions (take\n",
    "    #      in a data matrix and return a new matrix of the same type/size).  can also provide lists of\n",
    "    #      normalization functions to be applied separately to each data matrix.\n",
    "    #TODO: add support for multiple reduce functions (and/or reduce_kwargs)-- one per data matrix\n",
    "    #TODO: add hyperalignment and SRM.  also allow different subsets of the data to be aligned, e.g.\n",
    "    #      according to the unique IDs specified in a list of group IDs, some column of the data matrix,\n",
    "    #      cluster IDs, or a custom function (or a dictionary of functions specifying how different\n",
    "    #      group IDs should be aligned); functions should take in a list of data matrices and returned an aligned\n",
    "    #      list in the same format.\n",
    "    \n",
    "    style = style.lower()\n",
    "    \n",
    "    assert style in marker_styles + line_styles + combo_styles, f'Unsupported plot style: {style}'\n",
    "    \n",
    "    if is_combo(style):\n",
    "        mode='lines+markers'\n",
    "    elif is_line(style):\n",
    "        mode='lines'\n",
    "    else:\n",
    "        mode='markers'\n",
    "    \n",
    "    if is_line(style):\n",
    "        if not('line' in kwargs.keys()):\n",
    "                kwargs['line'] = {}\n",
    "        #if not('opacity' in kwargs['line'].keys()):\n",
    "        #    kwargs['line']['opacity'] = float(defaults['plot']['opacity'])\n",
    "        \n",
    "        dash = is_dashed(style)\n",
    "        if dash:\n",
    "            print(f'dashed line: {dash}')\n",
    "            if not('dash' in kwargs['line'].keys()):\n",
    "                kwargs['line']['dash'] = dash_styles[dash]\n",
    "    if is_marker(style):\n",
    "        if is_bigmarker(style):\n",
    "            size = int(defaults['plot']['bigmarkersize'])\n",
    "        else:\n",
    "            size = int(defaults['plot']['markersize'])\n",
    "        \n",
    "        if not('marker' in kwargs.keys()):\n",
    "            kwargs['marker'] = {}\n",
    "        if not('size' in kwargs['marker'].keys()):\n",
    "            if is_bigmarker(style):\n",
    "                kwargs['marker']['size'] = int(defaults['plot']['bigmarkersize'])\n",
    "            else:\n",
    "                kwargs['marker']['size'] = int(defaults['plot']['markersize'])        \n",
    "        if not('opacity' in kwargs['marker'].keys()):\n",
    "            kwargs['marker']['opacity'] = float(defaults['plot']['opacity'])            \n",
    "    \n",
    "    names = [str(n) for n in np.array(reduced_data.index.levels[0])]\n",
    "    \n",
    "    if reduced_data.shape[1] == 2:\n",
    "        shapes = [go.Scatter(x=d.iloc[:, 0], y=d.iloc[:, 1], mode=mode, name=names[i], **kwargs) for i, d in enumerate(split_data)]\n",
    "    elif reduced_data.shape[1] == 3:\n",
    "        shapes = [go.Scatter3d(x=d.iloc[:, 0], y=d.iloc[:, 1], z=d.iloc[:, 2], mode=mode, name=names[i], **kwargs) for i, d in enumerate(split_data)]\n",
    "    \n",
    "    if HYPERTOOLS_NOTEBOOK_MODE:\n",
    "        return iplot(shapes)\n",
    "    else:\n",
    "        return plot(shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_r = reduce(xs, algorithm='UMAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(xs, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(xs, style='-:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(xs, style='-o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(xs, style='.', reduce_kwargs={'ndims': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(xs, style='-', reduce_kwargs={'ndims': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
